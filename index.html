<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Gaze-Ball Demo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    html, body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      height: 100%;
    }

    #video {
      display: none;
    }

    #ball {
      position: absolute;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      background: #ff4757;
      pointer-events: none;
      transition: transform 0.05s linear;
    }

    #loading {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      font: 18px/1 sans-serif;
      color: #555;
    }

    #error {
      position: absolute;
      bottom: 10px;
      left: 10px;
      background: rgba(255,0,0,0.1);
      color: red;
      font-family: monospace;
      white-space: pre-wrap;
    }
  </style>
</head>
<body>
  <video id="video" autoplay playsinline muted></video>
  <div id="ball"></div>
  <div id="loading">Requesting cameraâ€¦</div>
  <pre id="error"></pre>

  <!-- MediaPipe FaceMesh only -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>

  <script>
    const videoEl = document.getElementById('video');
    const ball = document.getElementById('ball');
    const loader = document.getElementById('loading');
    const errorBox = document.getElementById('error');

    let faceMesh;

    function showError(err) {
      console.error(err);
      errorBox.textContent = (err && err.message) || err.toString();
    }

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        videoEl.srcObject = stream;

        await videoEl.play();
        loader.remove();
        startFaceTracking();
      } catch (err) {
        showError("Camera error: " + err.name + "\n" + err.message);
      }
    }

    async function startFaceTracking() {
      faceMesh = new window.FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        selfieMode: true,
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });

      faceMesh.onResults(onResults);

      // Start processing frames manually
      async function detect() {
        if (videoEl.readyState >= 2) {
          await faceMesh.send({ image: videoEl });
        }
        requestAnimationFrame(detect);
      }

      detect();
    }

    function onResults(results) {
      if (!results.multiFaceLandmarks?.length) return;

      const lm = results.multiFaceLandmarks[0];
      const iris = lm[468]; // Right iris

      const xN = iris.x;
      const yN = iris.y;

      const xPx = xN * (window.innerWidth - 40);
      const yPx = yN * (window.innerHeight - 40);

      ball.style.transform = `translate(${xPx}px, ${yPx}px)`;
    }

    // Start everything
    startCamera();
  </script>
</body>
</html>
